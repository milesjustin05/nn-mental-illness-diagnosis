# -*- coding: utf-8 -*-
"""maml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhLdqTq5897kkI0v6DnG6iwTvo39Kgx5
"""

import torch
import torch.nn as nn
import torch.optim as optim
from collections import OrderedDict

# ----- Model -----
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 1)

    def forward(self, x, params=None):
        # If params provided, use them instead of model's own weights
        if params is None:
            x = torch.relu(self.fc1(x))
            x = self.fc2(x)
        else:
            x = torch.relu(torch.nn.functional.linear(x, params['fc1.weight'], params['fc1.bias']))
            x = torch.nn.functional.linear(x, params['fc2.weight'], params['fc2.bias'])
        return x

# ----- Hyperparameters -----
inner_lr = 0.01
outer_lr = 0.001
num_inner_steps = 5
num_tasks = 10

# ----- Initialize -----
model = SimpleModel()
meta_optimizer = optim.Adam(model.parameters(), lr=outer_lr)
loss_fn = nn.MSELoss()

# ----- Task Generator -----
def generate_task():
    # Shared global "true" weights — give tasks correlation
    base_w = torch.ones(10, 1)  # global underlying relationship
    w = base_w + 0.1 * torch.randn(10, 1)  # small per-task variation
    b = 0.5 + 0.1 * torch.randn(1)
    x = torch.randn(20, 10)
    y = x @ w + b + 0.1 * torch.randn(20, 1)
    return x, y

# ----- Meta-Training Loop -----
for epoch in range(500):
    meta_loss = 0.0

    for task in range(num_tasks):
        # Generate train/val data for one task
        x_train, y_train = generate_task()
        x_val, y_val = generate_task()

        # 1️⃣ Compute gradients on training data (inner loop)
        pred = model(x_train)
        loss = loss_fn(pred, y_train)
        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)

        # Build adapted parameters manually
        adapted_params = OrderedDict()
        for (name, param), grad in zip(model.named_parameters(), grads):
            adapted_params[name] = param - inner_lr * grad

        # 2️⃣ Compute validation loss using adapted parameters
        val_pred = model(x_val, params=adapted_params)
        val_loss = loss_fn(val_pred, y_val)
        meta_loss += val_loss

    # Average across tasks
    meta_loss /= num_tasks

    # 3️⃣ Meta-update (outer loop)
    meta_optimizer.zero_grad()
    meta_loss.backward()
    meta_optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Meta Loss: {meta_loss.item():.4f}")

import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Hyperparameters
inner_lr = 0.01
outer_lr = 0.001
num_inner_steps = 5
num_tasks = 10

# Initialize the model
model = SimpleModel()
meta_optimizer = optim.Adam(model.parameters(), lr = outer_lr)

# Generate some dummy tasks (for demonstration purposes)
def generate_task():
    # Shared global "true" weights — give tasks correlation
    base_w = torch.ones(10, 1)  # global underlying relationship
    w = base_w + 0.1 * torch.randn(10, 1)  # small per-task variation
    b = 0.5 + 0.1 * torch.randn(1)
    x = torch.randn(20, 10)
    y = x @ w + b + 0.1 * torch.randn(20, 1)
    return x, y


# Meta - training loop
for epoch in range(500):
    meta_loss = 0
    for task in range(num_tasks):
        # Generate a task
        x_train, y_train = generate_task()
        x_val, y_val = generate_task()

        # Inner loop
        temp_model = SimpleModel()
        temp_model.load_state_dict(model.state_dict())
        inner_optimizer = optim.SGD(temp_model.parameters(), lr = inner_lr)
        for _ in range(num_inner_steps):
            pred = temp_model(x_train)
            loss = nn.MSELoss()(pred, y_train)
            inner_optimizer.zero_grad()
            loss.backward()
            inner_optimizer.step()

        # Outer loop
        val_pred = temp_model(x_val)
        val_loss = nn.MSELoss()(val_pred, y_val)
        meta_loss += val_loss

    meta_loss /= num_tasks
    meta_optimizer.zero_grad()
    meta_loss.backward()
    meta_optimizer.step()

    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Meta Loss: {meta_loss.item()}')

